{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc2b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle imports \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from pathlib import Path\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Reshape, Conv2DTranspose, MaxPooling2D, UpSampling2D, LeakyReLU\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2508e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset celeb_a (1.38 GiB) to C:\\Users\\ryanm\\tensorflow_datasets\\celeb_a\\0.3.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d475da859d94650a213957c6b48aace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5516f150694ba9aa18b269fae2e962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c9b37a8c9141cfb9f7c63e432df3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotFoundError",
     "evalue": "NewRandomAccessFile failed to Create/Open: C:/Users/ryanm/anaconda3/envs/SoftComputing/Lib/site-packages/tensorflow_datasets/downloads/img_align_celeba.zip : The system cannot find the path specified.\r\n; No such process",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4100/1804807644.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimg_data_dir\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/ryanm/anaconda3/envs/SoftComputing/Lib/site-packages/tensorflow_datasets/downloads\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mds_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'celeb_a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[1;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    298\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     \u001b[0mdbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[1;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[1;34m(self, download_dir, download_config)\u001b[0m\n\u001b[0;32m    283\u001b[0m       \u001b[1;31m# it to every sub function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtemporary_assignment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_data_dir\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_data_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         self._download_and_prepare(\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mdl_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             download_config=download_config)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_download_and_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m     \u001b[1;31m# Extract max_examples_per_split and forward it to _prepare_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[0m\u001b[0;32m    947\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[1;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[1;31m# Generating data for all splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[0msplit_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplits_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSplitDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0msplit_generator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_generators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0msplits_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mALL\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msplit_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\image\\celeba.py\u001b[0m in \u001b[0;36m_split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_split_generators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     extracted_dirs = dl_manager.download_and_extract({\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;34m\"img_align_celeba\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIMG_ALIGNED_DATA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;34m\"list_eval_partition\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mEVAL_LIST\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_map_promise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_extract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36m_map_promise\u001b[1;34m(map_fn, all_inputs)\u001b[0m\n\u001b[0;32m    393\u001b[0m   \u001b[1;34m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m   \u001b[0mall_promises\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_nested\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_inputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Apply the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m   \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_nested\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_wait_on_promise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[0;32m    125\u001b[0m   \u001b[1;31m# Could add support for more exotic data_struct, like OrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     return {\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmap_nested\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_struct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    126\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     return {\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmap_nested\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_struct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     }\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m   \u001b[1;31m# Singleton\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36m_wait_on_promise\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_wait_on_promise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36m_target_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;31m# type: (bool) -> Any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36m_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_raise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mraise_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                 \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraise_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36mhandle_future_result\u001b[1;34m(future)\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;31m# type: (Any) -> None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 844\u001b[1;33m             \u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    435\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\concurrent\\futures\\thread.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\download\\downloader.py\u001b[0m in \u001b[0;36m_sync_download\u001b[1;34m(self, url, destination_path)\u001b[0m\n\u001b[0;32m    206\u001b[0m       \u001b[1;31m# use requests (http) or urllib (ftp).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sync_file_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnimplementedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow_datasets\\core\\download\\downloader.py\u001b[0m in \u001b[0;36m_sync_file_copy\u001b[1;34m(self, filepath, destination_path)\u001b[0m\n\u001b[0;32m    139\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_sync_file_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mout_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestination_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     hexdigest, size = utils.read_checksum_digest(\n\u001b[0;32m    143\u001b[0m         out_path, checksum_cls=self._checksumer)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sc_final_project\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mcopy_v2\u001b[1;34m(src, dst, overwrite)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m   \"\"\"\n\u001b[1;32m--> 512\u001b[1;33m   _pywrap_file_io.CopyFile(\n\u001b[0m\u001b[0;32m    513\u001b[0m       compat.as_bytes(src), compat.as_bytes(dst), overwrite)\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: NewRandomAccessFile failed to Create/Open: C:/Users/ryanm/anaconda3/envs/SoftComputing/Lib/site-packages/tensorflow_datasets/downloads/img_align_celeba.zip : The system cannot find the path specified.\r\n; No such process"
     ]
    }
   ],
   "source": [
    "img_data_dir  = Path(\"C:/Users/ryanm/anaconda3/envs/SoftComputing/Lib/site-packages/tensorflow_datasets/downloads\")\n",
    "ds_train, ds_info = tfds.load('celeb_a', split='test', shuffle_files=False, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07552ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tfds.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['attributes'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "blond = features['attributes']['Blond_Hair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53157b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "batch_train = ds_train.batch(sample_size)\n",
    "features = next(iter(batch_train.take(1)))\n",
    "sample_images = features['image'][blond]\n",
    "new_image = np.median(sample_images, axis=0)\n",
    "plt.imshow(new_image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(req_attribs, features):\n",
    "    sample_bool = []\n",
    "    for i in range(sample_size):\n",
    "        match = True\n",
    "        for req_attrib in req_attribs:\n",
    "            if features['attributes'][req_attrib][i] == False:\n",
    "                match = False\n",
    "                break\n",
    "        sample_bool.append(match)\n",
    "    return features['image'][np.array(sample_bool, dtype=np.bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "male = sample({\"Male\":True}, features)\n",
    "female = sample({\"Male\":False}, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a61935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average male\n",
    "avg_male = np.mean(male, axis=0)\n",
    "plt.imshow(avg_male.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average female\n",
    "avg_female = np.mean(female, axis=0)\n",
    "plt.imshow(avg_female.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = features['image']\n",
    "rand_img = np.zeros(sample_images.shape[1:], dtype=np.uint8)\n",
    "for i in range(rand_img.shape[0])\n",
    "    for j in range(rand_img.shape[1]):\n",
    "        rand_int = np.random.randint(0, sample_images.shape[0])\n",
    "        new_image[i, j] = sample_images[rand_int, i, j]\n",
    "        \n",
    "plt.imshow(rand_img.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54790ee7",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65043cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.math.round(image / 255.)\n",
    "    return image, tf.cast(image, tf.int32)\n",
    "\n",
    "(mnist_train, mnist_test), mnist_info = tfds.load('mnist',\n",
    "                                                 split = ['test', 'test'],\n",
    "                                                 shuffle_files=True,\n",
    "                                                 as_supervised=True,\n",
    "                                                 with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2eb7b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1740ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, mask_type, kernel=5, filters=1):\n",
    "        super(MaskedConv2D, self).__init__()\n",
    "        self.kernel = kernel\n",
    "        self.filters= filters\n",
    "        self.mask_type = mask_type\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=[self.kernel,\n",
    "                                       self.kernel,\n",
    "                                       input_shape[-1],\n",
    "                                       self.filters],\n",
    "                                       initializer='glorot_normal',\n",
    "                                       trainable = True)\n",
    "        \n",
    "        self.b = self.add_weight(shape=(self.filters), initializer = 'zeros', trainable=True)\n",
    "                \n",
    "        mask = np.ones(self.kernel**2, dtype=np.float32)\n",
    "        center = len(mask) // 2\n",
    "        mask[center+1:] = 0\n",
    "        if self.mask_type == 'A':\n",
    "            mask[center]=0\n",
    "        mask = mask.reshape((self.kernel, self.kernel, 1, 1))\n",
    "        self.mask = tf.constant(mask, dtype='float32')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        masked_w = tf.math.multiply(self.w, self.mask)\n",
    "        output = tf.nn.conv2d(inputs, masked_w, 1 , \"SAME\") + self.b\n",
    "        return tf.nn.relu(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b52e1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, h=32):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.forward = Sequential([MaskedConv2D('B', kernel=1, filters=h),\n",
    "                                  MaskedConv2D('B', kernel=3, filters=h),\n",
    "                                  MaskedConv2D('B', kernel=1, filters=2*h)])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.forward(inputs)\n",
    "        return x + inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e09d2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimplePixelCnn(hidden_features=64,\n",
    "                  output_features=64,\n",
    "                  resblocks_num=7):\n",
    "    \n",
    "    inputs= layers.Input(shape=[28,28,1])\n",
    "    x = inputs\n",
    "    \n",
    "    x = MaskedConv2D('A', kernel=7, filters=2*hidden_features)(x)\n",
    "    \n",
    "    for _ in range(resblocks_num):\n",
    "        x = ResidualBlock(hidden_features)(x)\n",
    "        \n",
    "    x = layers.Conv2D(output_features, (1,1), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(1, (1,1), padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name='PixelCnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c972b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PixelCnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "masked_conv2d_22 (MaskedConv (None, 28, 28, 128)       6400      \n",
      "_________________________________________________________________\n",
      "residual_block_7 (ResidualBl (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_8 (ResidualBl (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_9 (ResidualBl (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_10 (ResidualB (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_11 (ResidualB (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_12 (ResidualB (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_13 (ResidualB (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        8256      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 1)         65        \n",
      "=================================================================\n",
      "Total params: 389,249\n",
      "Trainable params: 389,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pixelcnn = SimplePixelCnn()\n",
    "pixelcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test_), ds_info = tfds.load('mnist', \n",
    "                              split=['train', 'test'], \n",
    "                              shuffle_files=True,\n",
    "                              as_supervised=True,\n",
    "                              with_info=True)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255.\n",
    "    return image, image\n",
    "\n",
    "\n",
    "ds_train = ds_train.map(preprocess)\n",
    "ds_train = ds_train.cache() # put dataset into memory\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "\n",
    "ds_test = ds_test_.map(preprocess).batch(batch_size).cache().prefetch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_label(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.math.round(image/255.)\n",
    "    return image, label\n",
    "\n",
    "ds_test_label = ds_test_.map(preprocess_with_label).batch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(z_dim):\n",
    "    inputs  = layers.Input(shape=[28,28,1])\n",
    "    \n",
    "    x = inputs    \n",
    "    x = Conv2D(filters=8,  kernel_size=(3,3), strides=2, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(filters=8,  kernel_size=(3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(filters=8,  kernel_size=(3,3), strides=2, padding='same', activation='relu')(x)\n",
    "    x = Conv2D(filters=8,  kernel_size=(3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(z_dim)(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=out, name='encoder')\n",
    "\n",
    "def Decoder(z_dim):\n",
    "    inputs  = layers.Input(shape=[z_dim])\n",
    "    x = inputs    \n",
    "    x = Dense(7*7*64, activation='relu')(x)\n",
    "    x = Reshape((7,7,64))(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='same', activation='relu')(x)\n",
    "    x = UpSampling2D((2,2))(x)    \n",
    "\n",
    "    out = Conv2D(filters=1, kernel_size=(3,3), strides=1, padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "    #return out          \n",
    "    return Model(inputs=inputs, outputs=out, name='decoder')\n",
    "\n",
    "class Autoencoder:\n",
    "    def __init__(self, z_dim):\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "        \n",
    "        model_input = self.encoder.input\n",
    "        model_output = self.decoder(self.encoder.output)\n",
    "        self.model = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(z_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01640065",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/autoencoder.h5\"\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, \n",
    "                             monitor= \"val_loss\", \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode= \"auto\", \n",
    "                             save_weights_only = False)\n",
    "\n",
    "early = EarlyStopping(monitor= \"val_loss\", \n",
    "                      mode= \"auto\", \n",
    "                      patience = 5)\n",
    "\n",
    "callbacks_list = [checkpoint, early]\n",
    "\n",
    "autoencoder.model.compile(\n",
    "    loss = \"mse\",\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=3e-4))\n",
    "    #metrics=[tf.keras.losses.BinaryCrossentropy()])\n",
    "autoencoder.model.fit(ds_train, validation_data=ds_test,\n",
    "                epochs = 5, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2626c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(ds_test))\n",
    "autoencoder.model = load_model(model_path)\n",
    "outputs = autoencoder.model.predict(images)\n",
    "\n",
    "# Display\n",
    "grid_col = 10\n",
    "grid_row = 2\n",
    "\n",
    "f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col*1.1, grid_row))\n",
    "\n",
    "i = 0\n",
    "for row in range(0, grid_row, 2):\n",
    "    for col in range(grid_col):\n",
    "        axarr[row,col].imshow(images[i,:,:,0], cmap='gray')\n",
    "        axarr[row,col].axis('off')\n",
    "        axarr[row+1,col].imshow(outputs[i,:,:,0], cmap='gray')\n",
    "        axarr[row+1,col].axis('off')        \n",
    "        i += 1\n",
    "f.tight_layout(0.1, h_pad=0.2, w_pad=0.1)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825051ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_2 = Autoencoder(z_dim=2)\n",
    "\n",
    "early = EarlyStopping(monitor= \"val_loss\", \n",
    "                      mode= \"auto\", \n",
    "                      patience = 5)\n",
    "\n",
    "callbacks_list = [early]\n",
    "\n",
    "autoencoder_2.model.compile(\n",
    "    loss = \"mse\",\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3))\n",
    "\n",
    "autoencoder_2.model.fit(ds_train, validation_data=ds_test,\n",
    "                epochs = 5, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b5a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(ds_test_label))\n",
    "outputs = autoencoder_2.encoder.predict(images)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(outputs[:,0], outputs[:,1], c=labels, cmap='RdYlBu', s=3)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_samples = np.array([[z1, z2] for z2 in np.arange(-5, 5, 1.) for z1 in np.arange(-5, 5, 1.)])\n",
    "images = autoencoder_2.decoder.predict(z_samples)\n",
    "grid_col = 10\n",
    "grid_row = 10\n",
    "\n",
    "f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col, grid_row))\n",
    "\n",
    "i = 0\n",
    "for row in range(grid_row):\n",
    "    for col in range(grid_col):\n",
    "        axarr[row,col].imshow(images[i,:,:,0], cmap='gray')\n",
    "        axarr[row,col].axis('off')   \n",
    "        i += 1\n",
    "f.tight_layout(0.1, h_pad=0.2, w_pad=0.1)        \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55720e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Layer, Input, Conv2D, Dense, Flatten, Reshape, Lambda, Dropout\n",
    "from tensorflow.keras.layers import Conv2DTranspose, MaxPooling2D, UpSampling2D, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime, os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "num_devices = strategy.num_replicas_in_sync\n",
    "print('Number of devices: {}'.format(num_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128*num_devices\n",
    "\n",
    "def preprocess(sample):\n",
    "    image = sample['image']\n",
    "    image = tf.image.resize(image, [112,112])\n",
    "    image = tf.cast(image, tf.float32)/255.\n",
    "    return image, image\n",
    "\n",
    "ds_train = ds_train.map(preprocess)\n",
    "ds_train = ds_train.shuffle(batch_size*4)\n",
    "ds_train = ds_train.batch(batch_size).prefetch(batch_size)\n",
    "\n",
    "ds_test = ds_test_.map(preprocess).batch(batch_size).prefetch(batch_size)\n",
    "\n",
    "train_num = ds_info.splits['train'].num_examples\n",
    "test_num = ds_info.splits['test'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSampling(Layer):        \n",
    "    def call(self, inputs):\n",
    "        means, logvar = inputs\n",
    "        epsilon = tf.random.normal(shape=tf.shape(means), mean=0., stddev=1.)\n",
    "        samples = means + tf.exp(0.5*logvar)*epsilon\n",
    "\n",
    "        return samples\n",
    "    \n",
    "class DownConvBlock(Layer):\n",
    "    count = 0\n",
    "    def __init__(self, filters, kernel_size=(3,3), strides=1, padding='same'):\n",
    "        super(DownConvBlock, self).__init__(name=f\"DownConvBlock_{DownConvBlock.count}\")\n",
    "        DownConvBlock.count+=1\n",
    "        self.forward = Sequential([Conv2D(filters, kernel_size, strides, padding)])\n",
    "        self.forward.add(BatchNormalization())\n",
    "        self.forward.add(layers.LeakyReLU(0.2))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "class UpConvBlock(Layer):\n",
    "    count = 0\n",
    "    def __init__(self, filters, kernel_size=(3,3), padding='same'):\n",
    "        super(UpConvBlock, self).__init__(name=f\"UpConvBlock_{UpConvBlock.count}\")\n",
    "        UpConvBlock.count += 1\n",
    "        self.forward = Sequential([Conv2D(filters, kernel_size, 1, padding),])\n",
    "        self.forward.add(layers.LeakyReLU(0.2))\n",
    "        self.forward.add(UpSampling2D((2,2)))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "    \n",
    "class Encoder(Layer):\n",
    "    def __init__(self, z_dim, name='encoder'):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        \n",
    "        self.features_extract = Sequential([\n",
    "            DownConvBlock(filters = 32, kernel_size=(3,3), strides=2),\n",
    "            DownConvBlock(filters = 32, kernel_size=(3,3), strides=2),\n",
    "            DownConvBlock(filters = 64, kernel_size=(3,3), strides=2),\n",
    "            DownConvBlock(filters = 64, kernel_size=(3,3), strides=2),\n",
    "            Flatten()])\n",
    "        \n",
    "        self.dense_mean = Dense(z_dim, name='mean')\n",
    "        self.dense_logvar = Dense(z_dim, name='logvar')\n",
    "        self.sampler = GaussianSampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.features_extract(inputs)\n",
    "        mean = self.dense_mean(x)\n",
    "        logvar = self.dense_logvar(x)\n",
    "        z = self.sampler([mean, logvar])\n",
    "        return z, mean, logvar\n",
    "\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, z_dim, name='decoder'):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "            \n",
    "        self.forward = Sequential([\n",
    "                        Dense(7*7*64, activation='relu'),\n",
    "                        Reshape((7,7,64)),\n",
    "                        UpConvBlock(filters=64, kernel_size=(3,3)),\n",
    "                        UpConvBlock(filters=64, kernel_size=(3,3)),\n",
    "                        UpConvBlock(filters=32, kernel_size=(3,3)),\n",
    "                        UpConvBlock(filters=32, kernel_size=(3,3)),\n",
    "                        Conv2D(filters=3, kernel_size=(3,3), strides=1, padding='same', activation='sigmoid'),\n",
    "                \n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "    \n",
    "class VAE(Model):\n",
    "    def __init__(self, z_dim, name='VAE'):\n",
    "        super(VAE, self).__init__(name=name)\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "        self.mean = None\n",
    "        self.logvar = None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z, self.mean, self.logvar = self.encoder(inputs)\n",
    "        out = self.decoder(z)           \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a63886",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_devices>1:\n",
    "    with strategy.scope():\n",
    "        vae = VAE(z_dim=200)\n",
    "else:\n",
    "    vae = VAE(z_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_kl_loss(y_true, y_pred):\n",
    "    kl_loss =  - 0.5 * tf.reduce_mean(1 + vae.logvar - tf.square(vae.mean) - tf.exp(vae.logvar))\n",
    "    return kl_loss    \n",
    "\n",
    "def vae_rc_loss(y_true, y_pred):\n",
    "    #rc_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    rc_loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "    return rc_loss\n",
    "\n",
    "def vae_loss(y_true, y_pred):\n",
    "    kl_loss = vae_kl_loss(y_true, y_pred)\n",
    "    rc_loss = vae_rc_loss(y_true, y_pred)\n",
    "    kl_weight_const = 0.01\n",
    "    return kl_weight_const*kl_loss + rc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/my_vae_celeb_a.h5\"\n",
    "os.makedirs(\"./models\", exist_ok=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_path, \n",
    "                             monitor= \"vae_rc_loss\", \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode= \"auto\", \n",
    "                             save_weights_only = True)\n",
    "\n",
    "early = EarlyStopping(monitor= \"vae_rc_loss\", \n",
    "                      mode= \"auto\", \n",
    "                      patience = 3)\n",
    "\n",
    "callbacks_list = [checkpoint, early]\n",
    "\n",
    "initial_learning_rate = 1e-3\n",
    "steps_per_epoch = int(np.round(train_num/batch_size))\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=steps_per_epoch,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "vae.compile(\n",
    "    loss = [vae_loss],\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=3e-3),\n",
    "    metrics=[vae_kl_loss,vae_rc_loss])\n",
    "\n",
    "\n",
    "history = vae.fit(ds_train, validation_data=ds_test,\n",
    "                epochs = 5, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a323ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/vae_celeb_a.h5\"\n",
    "vae.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(ds_train))\n",
    "vae.load_weights(model_path)\n",
    "outputs = vae.predict(images)\n",
    "\n",
    "# Display\n",
    "grid_col = 8\n",
    "grid_row = 2\n",
    "\n",
    "f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col*2, grid_row*2))\n",
    "\n",
    "i = 0\n",
    "for row in range(0, grid_row, 2):\n",
    "    for col in range(grid_col):\n",
    "        axarr[row,col].imshow(images[i])\n",
    "        axarr[row,col].axis('off')\n",
    "        axarr[row+1,col].imshow(outputs[i])\n",
    "        axarr[row+1,col].axis('off')        \n",
    "        i += 1\n",
    "f.tight_layout(0.1, h_pad=0.2, w_pad=0.1)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6745cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_file = True # Set to False to calculate the statistic but it can be slow\n",
    "\n",
    "if load_from_file:\n",
    "    stats = pickle.load(open('stats.p','wb'))\n",
    "    avg_z_mean = stats['avg_z_mean']\n",
    "    avg_z_std = stats['avg_z_std']\n",
    "else:    \n",
    "    avg_z_mean = []\n",
    "    avg_z_std = []\n",
    "    for i in range(steps_per_epoch):\n",
    "        images, labels = next(iter(ds_train))    \n",
    "        z, z_mean, z_logvar = vae.encoder(images)\n",
    "        avg_z_mean.append(np.mean(z_mean, axis=0))\n",
    "        avg_z_std.append(np.mean(np.exp(0.5*z_logvar),axis=0))\n",
    "\n",
    "    avg_z_mean = np.mean(avg_z_mean, axis=0)\n",
    "    avg_z_std = np.mean(avg_z_std, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da39205",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 200\n",
    "z_samples = np.random.normal(loc=0, scale=1, size=(25, z_dim))\n",
    "images = vae.decoder(z_samples.astype(np.float32))\n",
    "grid_col = 7\n",
    "grid_row = 2\n",
    "\n",
    "f, axarr = plt.subplots(grid_row, grid_col, figsize=(2*grid_col, 2*grid_row))\n",
    "\n",
    "i = 0\n",
    "for row in range(grid_row):\n",
    "    for col in range(grid_col):\n",
    "        axarr[row,col].imshow(images[i])\n",
    "        axarr[row,col].axis('off')   \n",
    "        i += 1\n",
    "f.tight_layout(0.1, h_pad=0.2, w_pad=0.1)        \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69bfaf9",
   "metadata": {},
   "source": [
    "## Problem 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a4016",
   "metadata": {},
   "source": [
    "### Part a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94dcb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload mnist dataset\n",
    "(ds_train, ds_test_), ds_info = tfds.load('mnist', \n",
    "                              split=['train', 'test'], \n",
    "                              shuffle_files=True,\n",
    "                              as_supervised=True,\n",
    "                              with_info=True)\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/255.\n",
    "    return image, image\n",
    "\n",
    "def preprocess_with_label(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.math.round(image/255.)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "ds_train = ds_train.map(preprocess)\n",
    "ds_train = ds_train.cache() # put dataset into memory\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "\n",
    "ds_test = ds_test_.map(preprocess).batch(batch_size).cache().prefetch(batch_size)\n",
    "ds_test_label = ds_test.map(preprocess_with_label).batch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dcdc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, mask_type, kernel=5, filters=1):\n",
    "        super(MaskedConv2D, self).__init__()\n",
    "        self.kernel = kernel\n",
    "        self.filters= filters\n",
    "        self.mask_type = mask_type\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=[self.kernel,\n",
    "                                       self.kernel,\n",
    "                                       input_shape[-1],\n",
    "                                       self.filters],\n",
    "                                       initializer='glorot_normal',\n",
    "                                       trainable = True)\n",
    "        \n",
    "        self.b = self.add_weight(shape=(self.filters), initializer = 'zeros', trainable=True)\n",
    "                \n",
    "        mask = np.ones(self.kernel**2, dtype=np.float32)\n",
    "        center = len(mask) // 2\n",
    "        mask[center+1:] = 0\n",
    "        if self.mask_type == 'A':\n",
    "            mask[center]=0\n",
    "        mask = mask.reshape((self.kernel, self.kernel, 1, 1))\n",
    "        self.mask = tf.constant(mask, dtype='float32')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        masked_w = tf.math.multiply(self.w, self.mask)\n",
    "        output = tf.nn.conv2d(inputs, masked_w, 1 , \"SAME\") + self.b\n",
    "        return tf.nn.relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47a0b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_label(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.math.round(image/255.)\n",
    "    return image, label\n",
    "\n",
    "ds_test_label = ds_test_.map(preprocess_with_label).batch(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b54230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, h=32):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.forward = Sequential([MaskedConv2D('B', kernel=1, filters=h),\n",
    "                                  MaskedConv2D('B', kernel=3, filters=h),\n",
    "                                  MaskedConv2D('B', kernel=1, filters=2*h)])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.forward(inputs)\n",
    "        return x + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf8205dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimplePixelCnn(hidden_features=64,\n",
    "                  output_features=64,\n",
    "                  resblocks_num=7):\n",
    "    \n",
    "    inputs= layers.Input(shape=[28,28,1])\n",
    "    x = inputs\n",
    "    \n",
    "    x = MaskedConv2D('A', kernel=7, filters=2*hidden_features)(x)\n",
    "    \n",
    "    for _ in range(resblocks_num):\n",
    "        x = ResidualBlock(hidden_features)(x)\n",
    "        \n",
    "    x = layers.Conv2D(output_features,(1,1), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(1, (1,1), padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name='PixelCnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae15448",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelcnn = SimplePixelCnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0b618",
   "metadata": {},
   "source": [
    "### Part b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4abcef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"PixelCnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "masked_conv2d_22 (MaskedConv (None, 28, 28, 128)       6400      \n",
      "_________________________________________________________________\n",
      "residual_block_7 (ResidualBl (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_8 (ResidualBl (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_9 (ResidualBl (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_10 (ResidualB (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_11 (ResidualB (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_12 (ResidualB (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "residual_block_13 (ResidualB (None, 28, 28, 128)       53504     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        8256      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 1)         65        \n",
      "=================================================================\n",
      "Total params: 389,249\n",
      "Trainable params: 389,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pixelcnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683036f",
   "metadata": {},
   "source": [
    "### Part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f9742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 824s 4s/step - loss: 0.1191 - binary_crossentropy: 0.1191 - val_loss: 0.0899 - val_binary_crossentropy: 0.0899\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 824s 4s/step - loss: 0.0861 - binary_crossentropy: 0.0861 - val_loss: 0.0818 - val_binary_crossentropy: 0.0818\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 829s 4s/step - loss: 0.0828 - binary_crossentropy: 0.0828 - val_loss: 0.0837 - val_binary_crossentropy: 0.0837\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 825s 4s/step - loss: 0.0812 - binary_crossentropy: 0.0812 - val_loss: 0.0796 - val_binary_crossentropy: 0.0796\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 825s 4s/step - loss: 0.0801 - binary_crossentropy: 0.0801 - val_loss: 0.0811 - val_binary_crossentropy: 0.0811\n",
      "Epoch 6/10\n",
      "165/235 [====================>.........] - ETA: 3:53 - loss: 0.0796 - binary_crossentropy: 0.0796"
     ]
    }
   ],
   "source": [
    "pixelcnn.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "    metrics = [tf.keras.metrics.BinaryCrossentropy()]\n",
    ")\n",
    "\n",
    "pixelcnn.fit(ds_train, epochs = 10, validation_data=ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 25\n",
    "h = 28\n",
    "w = 28\n",
    "images = np.zeros((batches,h,w,1), dtype=np.float32)\n",
    "\n",
    "for row in range(h):\n",
    "\n",
    "    for col in range(w):\n",
    "\n",
    "        prob = pixel_cnn.predict(images)[:,row,col,0]\n",
    "\n",
    "        pixel_samples = tf.random.categorical(tf.math.log(np.stack([1-prob, prob],1)), 1)\n",
    "\n",
    "        images[:,row,col,0] = tf.reshape(pixel_samples,[batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0099a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the 25 images\n",
    "f, axarr = plt.subplots(5, 5, figsize=(5*1.1,5))\n",
    "\n",
    "i = 0\n",
    "for row in range(5):\n",
    "    for col in range(5):\n",
    "        axarr[row,col].imshow(images[i,:,:,0], cmap='gray')\n",
    "        axarr[row,col].axis('off')\n",
    "        i += 1\n",
    "f.tight_layout(0.1, h_pad=0.2, w_pad=0.1)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd9663",
   "metadata": {},
   "source": [
    "### Part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelcnn.fit(ds_train, epochs = 25, validation_data=ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 25\n",
    "h = 28\n",
    "w = 28\n",
    "images = np.zeros((batches,h,w,1), dtype=np.float32)\n",
    "\n",
    "for row in range(h):\n",
    "\n",
    "    for col in range(w):\n",
    "\n",
    "        prob = pixel_cnn.predict(images)[:,row,col,0]\n",
    "\n",
    "        pixel_samples = tf.random.categorical(tf.math.log(np.stack([1-prob, prob],1)), 1)\n",
    "\n",
    "        images[:,row,col,0] = tf.reshape(pixel_samples,[batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the 25 images\n",
    "f, axarr = plt.subplots(5, 5, figsize=(5*1.1,5))\n",
    "\n",
    "i = 0\n",
    "for row in range(5):\n",
    "    for col in range(5):\n",
    "        axarr[row,col].imshow(images[i,:,:,0], cmap='gray')\n",
    "        axarr[row,col].axis('off')\n",
    "        i += 1\n",
    "f.tight_layout(0.1, h_pad=0.2, w_pad=0.1)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e4f5e",
   "metadata": {},
   "source": [
    "### Part e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e26cb",
   "metadata": {},
   "source": [
    "analysis on part e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386a0a2",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75025c21",
   "metadata": {},
   "source": [
    "### Part a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self, generator, discriminator):        \n",
    "        # discriminator\n",
    "        self.D = discriminator\n",
    "        self.G = generator\n",
    "\n",
    "        self.bce = tf.keras.losses.BinaryCrossentropy()\n",
    "        self.d_loss = {}\n",
    "        self.g_loss = {}\n",
    "        self.accuracy = {}        \n",
    "        self.g_gradients = []\n",
    "\n",
    "    def discriminator_loss(self, pred_fake, pred_real):\n",
    "        real_loss = self.bce(tf.ones_like(pred_real), pred_real)\n",
    "        fake_loss = self.bce(tf.zeros_like(pred_fake), pred_fake)\n",
    "        \n",
    "        d_loss = 0.5*(real_loss + fake_loss)\n",
    "        return d_loss\n",
    "    \n",
    "    def generator_loss(self, pred_fake):\n",
    "        g_loss = self.bce(tf.ones_like(pred_fake), pred_fake)\n",
    "        return g_loss\n",
    "    \n",
    "    def train_step(self, g_input, real_input):\n",
    "\n",
    "        with tf.GradientTape() as g_tape,\\\n",
    "             tf.GradientTape() as d_tape:\n",
    "            # Feed forward\n",
    "            fake_input = self.G(g_input)\n",
    "\n",
    "            pred_fake = self.D(fake_input)\n",
    "            pred_real = self.D(real_input)\n",
    "\n",
    "            # Calculate losses\n",
    "            d_loss = self.discriminator_loss(pred_fake, pred_real)\n",
    "            g_loss = self.generator_loss(pred_fake)\n",
    "            \n",
    "            # Accuracy\n",
    "            fake_accuracy = tf.math.reduce_mean(binary_accuracy(tf.zeros_like(pred_fake), pred_fake))\n",
    "            real_accuracy = tf.math.reduce_mean(binary_accuracy(tf.ones_like(pred_real), pred_real))\n",
    "            \n",
    "            # backprop gradients\n",
    "            gradient_g = g_tape.gradient(g_loss, self.G.trainable_variables)\n",
    "            gradient_d = d_tape.gradient(d_loss, self.D.trainable_variables)\n",
    "            \n",
    "            gradient_g_l1_norm = [tf.norm(gradient).numpy() for gradient in gradient_g]\n",
    "            self.g_gradients.append(gradient_g_l1_norm) \n",
    "            # update weights\n",
    "            self.G_optimizer.apply_gradients(zip(gradient_g, self.G.trainable_variables))\n",
    "            self.D_optimizer.apply_gradients(zip(gradient_d, self.D.trainable_variables))\n",
    "\n",
    "\n",
    "        return g_loss, d_loss, fake_accuracy, real_accuracy\n",
    "    \n",
    "    def train(self, data_generator, \n",
    "                    z_generator,\n",
    "                    g_optimizer, d_optimizer,\n",
    "                    steps, interval=100):\n",
    "        self.D_optimizer = d_optimizer\n",
    "        self.G_optimizer = g_optimizer          \n",
    "        val_g_input = next(z_generator)\n",
    "        for i in range(steps):\n",
    "            g_input = next(z_generator)\n",
    "            real_input = next(data_generator)\n",
    "            \n",
    "            g_loss, d_loss, fake_accuracy, real_accuracy = self.train_step(g_input, real_input)\n",
    "            self.d_loss[i] = d_loss.numpy()\n",
    "            self.g_loss[i] = g_loss.numpy()\n",
    "            self.accuracy[i] = 0.5*(fake_accuracy.numpy() + real_accuracy.numpy())\n",
    "            if i%interval == 0:\n",
    "                msg = \"Step {}: d_loss {:.4f} g_loss {:.4f} Accuracy. real : {:.3f} fake : {:.3f}\"\\\n",
    "                .format(i, d_loss, g_loss, real_accuracy, fake_accuracy)\n",
    "                print(msg)\n",
    "                \n",
    "                fake_images = self.G(val_g_input)\n",
    "                self.plot_images(fake_images)\n",
    "\n",
    "    def plot_images(self, images):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb88a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(GAN):\n",
    "    def __init__(self, z_dim, input_shape):\n",
    "        \n",
    "        discriminator = self.Discriminator(input_shape)\n",
    "        generator = self.Generator(z_dim)\n",
    "        \n",
    "        GAN.__init__(self, generator, discriminator)\n",
    "        \n",
    "    def Discriminator(self, input_shape): \n",
    "\n",
    "        model = tf.keras.Sequential(name='Discriminator') \n",
    "        model.add(layers.Input(shape=input_shape)) \n",
    "\n",
    "        model.add(layers.Conv2D(32, 7, strides=(2,2), padding='same'))\n",
    "        model.add(layers.BatchNormalization(momentum=0.9))\n",
    "        model.add(layers.LeakyReLU(0.2)) \n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "        model.add(layers.Conv2D(64, 7, strides=(2,2), padding='same')) \n",
    "        model.add(layers.BatchNormalization(momentum=0.9)) \n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "        model.add(layers.Dropout(0.2))\n",
    "\n",
    "        model.add(layers.Flatten()) \n",
    "        model.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "        return model \n",
    "\n",
    "    def Generator(self, z_dim): \n",
    "\n",
    "        model = tf.keras.Sequential(name='Generator') \n",
    "        model.add(layers.Input(shape=[z_dim])) \n",
    "\n",
    "        model.add(layers.Dense(7*7*64))        \n",
    "        model.add(layers.BatchNormalization(momentum=0.9)) \n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "        model.add(layers.Reshape((7,7,64))) \n",
    "\n",
    "        model.add(layers.Conv2D(64, 7, padding='same')) \n",
    "        model.add(layers.BatchNormalization(momentum=0.9)) \n",
    "        model.add(layers.LeakyReLU(0.2))         \n",
    "        model.add(layers.UpSampling2D((2,2), interpolation=\"bilinear\"))\n",
    "\n",
    "        model.add(layers.Conv2D(32, 7, padding='same')) \n",
    "        model.add(layers.ReLU()) \n",
    "        model.add(layers.UpSampling2D((2,2), interpolation=\"bilinear\")) \n",
    "\n",
    "        model.add(layers.Conv2D(image_shape[-1], 3, padding='same', activation='tanh')) \n",
    "\n",
    "        return model     \n",
    "    \n",
    "    def plot_images(self, images):   \n",
    "        grid_row = 1\n",
    "        grid_col = 8\n",
    "        f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col*1.5, grid_row*1.5))\n",
    "        for col in range(grid_col):\n",
    "            axarr[col].imshow((images[col,:,:,0]+1)/2, cmap='gray')\n",
    "            axarr[col].axis('off') \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "\n",
    "def z_generator(batch_size, z_dim):\n",
    "    while True:\n",
    "         yield tf.random.normal((batch_size, z_dim))        \n",
    "            \n",
    "z_gen = z_generator(batch_size, z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "z_dim = 100\n",
    "image_shape = (28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise image \n",
    "noise_image = tf.random.normal((28*28, 100))\n",
    "noise_image = noise_image.reshape((28,28))\n",
    "plt.imshow(noise_image, cmap='gray'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_info = tfds.load('mnist', split='train', shuffle_files=True, with_info=True)\n",
    "ds_train = ds_train.map(preprocess)\n",
    "ds_train = ds_train.cache() # put dataset into memory\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size).repeat()\n",
    "train_num = ds_info.splits['train'].num_examples\n",
    "train_steps_per_epoch = round(train_num/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42760bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_gan = DCGAN(z_dim, image_shape)\n",
    "mnist_gan.D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb03173",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_gan.G.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc245674",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "#the display component is included in the train method\n",
    "mnist_gan.train(iter(ds_train), z_gen, \n",
    "          Adam(2e-3), Adam(2e-3),\n",
    "          num_epochs*train_steps_per_epoch, \n",
    "          1*train_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "#the display component is included in the train method\n",
    "mnist_gan.train(iter(ds_train), z_gen, \n",
    "          Adam(2e-3), Adam(2e-3),\n",
    "          num_epochs*train_steps_per_epoch, \n",
    "          2*train_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36db97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
